



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="author" content="PyTorch-Ignite Contributors">
  <meta name="creator" content="PyTorch-Ignite Contributors">
  <meta name="publisher" content="PyTorch-Ignite Contributors">
  <meta name="generator" content="Sphinx 5.3.0">
  <meta name="description" content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.">
  <meta name="keywords" content="pytorch, pytorch ignite, ignite, engine, events, handlers, metrics">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#ee4c2c">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@pytorch_ignite">
  <meta name="twitter:creator" content="@pytorch_ignite">
  <meta name="twitter:description" content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.">
  <meta name="twitter:title" content="ignite.distributed.auto &mdash; PyTorch-Ignite master (0f961480) Documentation">
  <meta name="twitter:image" content="https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logo.png">
  <meta name="twitter:image:alt" content="PyTorch-Ignite logo">

  <meta property="og:title" content="ignite.distributed.auto &mdash; PyTorch-Ignite master (0f961480) Documentation">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://pytorch.org/ignite/">
  <meta property="og:site_name" content="PyTorch-Ignite">
  <meta property="og:image" content="https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logo.png">
  <meta property="og:image:alt" content="PyTorch-Ignite logo">
  <meta property="og:description" content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.">

  <link rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin />

  
  <title>ignite.distributed.auto &mdash; PyTorch-Ignite master (0f961480) Documentation</title>
  

  
  
    <link rel="shortcut icon" type="image/svg+xml" href="../../../_static/ignite_logomark.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch.org/ignite/_modules/ignite/distributed/auto.html"/>
  

  

  
  
    

  

  <link rel="preload" href="../../../_static/css/theme.css" as="style" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="preload" href="../../../_static/pygments.css" as="style" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="preload" href="../../../_static/css/theme.css" as="style" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="preload" href="../../../_static/copybutton.css" as="style" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="preload" href="../../../_static/togglebutton.css" as="style" />
  <link rel="stylesheet" href="../../../_static/togglebutton.css" type="text/css" />
  <link rel="preload" href="../../../_static/banner.css" as="style" />
  <link rel="stylesheet" href="../../../_static/banner.css" type="text/css" />
  <link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" as="style" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" type="text/css" />
  <link rel="preload" href="../../../_static/katex-math.css" as="style" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
  <link rel="preload" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" as="style" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
    <link rel="preload" href="../../../_static/css/ignite_theme.css" as="style" />
    <link rel="stylesheet" href="../../../_static/css/ignite_theme.css" type="text/css" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3" as="style" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  <!-- katex links / this needs to be loaded before fonts.html -->

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous">
<script async src="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.js" crossorigin="anonymous"></script>

<!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5FELLLFHCP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5FELLLFHCP');
  </script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">

    <a class="header-logo" href="/ignite" aria-label="PyTorch-Ignite"></a>

    <div class="header-container">

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch-ignite.ai/tutorials/beginner/01-getting-started/">Quickstart</a>
          </li>

          <li>
            <a href="https://pytorch-ignite.ai/concepts/">Concepts</a>
          </li>

          <!-- <li>
            <a href="https://pytorch-ignite.ai/tutorials/beginner/01-getting-started/#complete-code">Examples</a>
          </li> -->

          <li>
            <a href="https://pytorch-ignite.ai/how-to-guides/">FAQ</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/ignite" target="_blank" rel="noopener noreferrer">GitHub</a>
          </li>

          <li>
            <a href="https://pytorch-ignite.ai/about/community/">About us</a>
          </li>

          <li>
            <a href="https://pytorch-ignite.ai">‚ä≥ pytorch-ignite.ai</a>
          </li>


        </ul>
      </div>

      <!-- <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a> -->
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <div class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></div>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (0f961480)
                </div>
              
            

            <div id="docsearch"></div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../engine.html">ignite.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../handlers.html">ignite.handlers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../metrics.html">ignite.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">ignite.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../exceptions.html">ignite.exceptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../utils.html">ignite.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contrib Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contrib/engines.html">ignite.contrib.engines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contrib/metrics.html">ignite.contrib.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contrib/handlers.html">ignite.contrib.handlers</a></li>
</ul>

            
          
        </div>
      </div>

      <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        <span class="fa fa-book"> Other Versions</span>
        v: master
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl>
            <dt>Tags</dt>
            <dd><a href="../../../../v0.3.0/index.html">v0.3.0</a></dd>
            <dd><a href="../../../../v0.4.0.post1/index.html">v0.4.0.post1</a></dd>
            <dd><a href="../../../../v0.4.1/index.html">v0.4.1</a></dd>
            <dd><a href="../../../../v0.4.10/index.html">v0.4.10</a></dd>
            <dd><a href="../../../../v0.4.11/index.html">v0.4.11</a></dd>
            <dd><a href="../../../../v0.4.12/index.html">v0.4.12</a></dd>
            <dd><a href="../../../../v0.4.13/index.html">v0.4.13</a></dd>
            <dd><a href="../../../../v0.4.2/index.html">v0.4.2</a></dd>
            <dd><a href="../../../../v0.4.3/index.html">v0.4.3</a></dd>
            <dd><a href="../../../../v0.4.4.post1/index.html">v0.4.4.post1</a></dd>
            <dd><a href="../../../../v0.4.5/index.html">v0.4.5</a></dd>
            <dd><a href="../../../../v0.4.6/index.html">v0.4.6</a></dd>
            <dd><a href="../../../../v0.4.7/index.html">v0.4.7</a></dd>
            <dd><a href="../../../../v0.4.8/index.html">v0.4.8</a></dd>
            <dd><a href="../../../../v0.4.9/index.html">v0.4.9</a></dd>
            <dd><a href="../../../../v0.4rc.0.post1/index.html">v0.4rc.0.post1</a></dd>
            <dd><a href="../../../../v0.5.0.post2/index.html">v0.5.0.post2</a></dd>
            <dd><a href="../../../../v0.5.1/index.html">v0.5.1</a></dd>
            <dd><a href="../../../../v0.5.2/index.html">v0.5.2</a></dd>
            <dd><a href="../../../../v0.5.3/index.html">v0.5.3</a></dd>
        </dl>
        <dl>
            <dt>Branches</dt>
            <dd><a href="auto.html">master</a></dd>
        </dl>
    </div>
</div>

    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>ignite.distributed.auto</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for ignite.distributed.auto</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim.optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">IterableDataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.sampler</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sampler</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">ignite.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">utils</span> <span class="k">as</span> <span class="n">idist</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ignite.distributed.comp_models</span><span class="w"> </span><span class="kn">import</span> <span class="n">horovod</span> <span class="k">as</span> <span class="n">idist_hvd</span><span class="p">,</span> <span class="n">native</span> <span class="k">as</span> <span class="n">idist_native</span><span class="p">,</span> <span class="n">xla</span> <span class="k">as</span> <span class="n">idist_xla</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">ignite.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup_logger</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;auto_dataloader&quot;</span><span class="p">,</span> <span class="s2">&quot;auto_model&quot;</span><span class="p">,</span> <span class="s2">&quot;auto_optim&quot;</span><span class="p">,</span> <span class="s2">&quot;DistributedProxySampler&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="auto_dataloader"><a class="viewcode-back" href="../../../generated/ignite.distributed.auto.auto_dataloader.html#ignite.distributed.auto.auto_dataloader">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">auto_dataloader</span><span class="p">(</span><span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">DataLoader</span><span class="p">,</span> <span class="s2">&quot;_MpDeviceLoader&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper method to create a dataloader adapted for non-distributed and distributed configurations (supporting</span>
<span class="sd">    all available backends from :meth:`~ignite.distributed.utils.available_backends()`).</span>

<span class="sd">    Internally, we create a dataloader with provided kwargs while applying the following updates:</span>

<span class="sd">    - batch size is scaled by world size: ``batch_size / world_size`` if larger or equal world size.</span>
<span class="sd">    - number of workers is scaled by number of local processes: ``num_workers / nprocs`` if larger or equal world size.</span>
<span class="sd">    - if no sampler provided by user, a `torch DistributedSampler`_ is setup.</span>
<span class="sd">    - if a `torch DistributedSampler`_ is provided by user, it is used without wrapping it.</span>
<span class="sd">    - if another sampler is provided, it is wrapped by :class:`~ignite.distributed.auto.DistributedProxySampler`.</span>
<span class="sd">    - if the default device is &#39;cuda&#39;, `pin_memory` is automatically set to `True`.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        Custom batch sampler is not adapted for distributed configuration. Please, make sure that provided batch</span>
<span class="sd">        sampler is compatible with distributed configuration.</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset: input torch dataset. If input dataset is `torch IterableDataset`_ then dataloader will be</span>
<span class="sd">            created without any distributed sampling. Please, make sure that the dataset itself produces</span>
<span class="sd">            different data on different ranks.</span>
<span class="sd">        kwargs: keyword arguments for `torch DataLoader`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `torch DataLoader`_ or `XLA MpDeviceLoader`_ for XLA devices</span>

<span class="sd">    Examples:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            import ignite.distribted as idist</span>

<span class="sd">            train_loader = idist.auto_dataloader(</span>
<span class="sd">                train_dataset,</span>
<span class="sd">                batch_size=32,</span>
<span class="sd">                num_workers=4,</span>
<span class="sd">                shuffle=True,</span>
<span class="sd">                pin_memory=&quot;cuda&quot; in idist.device().type,</span>
<span class="sd">                drop_last=True,</span>
<span class="sd">            )</span>

<span class="sd">    .. _torch DataLoader: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader</span>
<span class="sd">    .. _XLA MpDeviceLoader:</span>
<span class="sd">        https://pytorch.org/xla/release/2.0/index.html#running-on-multiple-xla-devices-with-multi-processing</span>
<span class="sd">    .. _torch DistributedSampler:</span>
<span class="sd">        https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler</span>
<span class="sd">    .. _torch IterableDataset: https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">get_rank</span><span class="p">()</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span>

    <span class="n">logger</span> <span class="o">=</span> <span class="n">setup_logger</span><span class="p">(</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.auto_dataloader&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;batch_size&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">world_size</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">//=</span> <span class="n">world_size</span>

        <span class="n">nproc</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">get_nproc_per_node</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;num_workers&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">nproc</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;num_workers&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">nproc</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">nproc</span>

        <span class="k">if</span> <span class="s2">&quot;batch_sampler&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;Found iterable dataset, dataloader will be created without any distributed sampling. &quot;</span>
                    <span class="s2">&quot;Please, make sure that the dataset itself produces different data on different ranks.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sampler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DistributedProxySampler</span><span class="p">,</span> <span class="n">DistributedSampler</span><span class="p">,</span> <span class="n">Sampler</span><span class="p">]]</span>
                <span class="n">sampler</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sampler&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">DistributedSampler</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">sampler</span><span class="o">.</span><span class="n">rank</span> <span class="o">!=</span> <span class="n">rank</span><span class="p">:</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found distributed sampler with rank=</span><span class="si">{</span><span class="n">sampler</span><span class="o">.</span><span class="n">rank</span><span class="si">}</span><span class="s2">, but process rank is </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">sampler</span><span class="o">.</span><span class="n">num_replicas</span> <span class="o">!=</span> <span class="n">world_size</span><span class="p">:</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Found distributed sampler with num_replicas=</span><span class="si">{</span><span class="n">sampler</span><span class="o">.</span><span class="n">num_replicas</span><span class="si">}</span><span class="s2">, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but world size is </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                <span class="k">elif</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># removes &quot;shuffle&quot; from kwargs if sampler is used</span>
                    <span class="n">shuffle</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;shuffle&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedProxySampler</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">)</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;sampler&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;Found batch_sampler in provided kwargs. Please, make sure that it is compatible &quot;</span>
                <span class="s2">&quot;with distributed configuration&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_xla_support</span> <span class="ow">and</span> <span class="n">idist</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="n">idist_xla</span><span class="o">.</span><span class="n">XLA_TPU</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pin_memory&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="c1"># TODO: How about XLA GPU ?</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Found incompatible options: xla support and pin_memory args equal True. &quot;</span>
            <span class="s2">&quot;Argument `pin_memory=False` will be used to construct data loader.&quot;</span>
        <span class="p">)</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;pin_memory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;pin_memory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pin_memory&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Use data loader kwargs for dataset &#39;</span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">dataset</span><span class="p">)[:</span><span class="mi">20</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="si">}</span><span class="s2">&#39;: </span><span class="se">\n\t</span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_xla_support</span> <span class="ow">and</span> <span class="n">idist</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span> <span class="o">==</span> <span class="n">idist_xla</span><span class="o">.</span><span class="n">XLA_TPU</span> <span class="ow">and</span> <span class="n">world_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;DataLoader is wrapped by `MpDeviceLoader` on XLA&quot;</span><span class="p">)</span>

        <span class="n">mp_device_loader_cls</span> <span class="o">=</span> <span class="n">_MpDeviceLoader</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">torch_xla.distributed.parallel_loader</span><span class="w"> </span><span class="kn">import</span> <span class="n">MpDeviceLoader</span>

            <span class="n">mp_device_loader_cls</span> <span class="o">=</span> <span class="n">MpDeviceLoader</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="n">mp_dataloader</span> <span class="o">=</span> <span class="n">mp_device_loader_cls</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">())</span>
        <span class="n">mp_dataloader</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">dataloader</span><span class="o">.</span><span class="n">sampler</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">return</span> <span class="n">mp_dataloader</span>

    <span class="k">return</span> <span class="n">dataloader</span></div>


<div class="viewcode-block" id="auto_model"><a class="viewcode-back" href="../../../generated/ignite.distributed.auto.auto_model.html#ignite.distributed.auto.auto_model">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">auto_model</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">sync_bn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper method to adapt provided model for non-distributed and distributed configurations (supporting</span>
<span class="sd">    all available backends from :meth:`~ignite.distributed.utils.available_backends()`).</span>

<span class="sd">    Internally, we perform to following:</span>

<span class="sd">    - send model to current :meth:`~ignite.distributed.utils.device()` if model&#39;s parameters are not on the device.</span>
<span class="sd">    - wrap the model to `torch DistributedDataParallel`_ for native torch distributed if world size is larger than 1.</span>
<span class="sd">    - wrap the model to `torch DataParallel`_ if no distributed context found and more than one CUDA devices available.</span>
<span class="sd">    - broadcast the initial variable states from rank 0 to all other processes if Horovod distributed framework is used.</span>

<span class="sd">    Args:</span>
<span class="sd">        model: model to adapt.</span>
<span class="sd">        sync_bn: if True, applies `torch convert_sync_batchnorm`_ to the model for native torch</span>
<span class="sd">            distributed only. Default, False. Note, if using Nvidia/Apex, batchnorm conversion should be</span>
<span class="sd">            applied before calling ``amp.initialize``.</span>
<span class="sd">        kwargs: kwargs to model&#39;s wrapping class: `torch DistributedDataParallel`_ or `torch DataParallel`_</span>
<span class="sd">            if applicable. Please, make sure to use acceptable kwargs for given backend.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.nn.Module</span>

<span class="sd">    Examples:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            import ignite.distribted as idist</span>

<span class="sd">            model = idist.auto_model(model)</span>

<span class="sd">        In addition with NVidia/Apex, it can be used in the following way:</span>

<span class="sd">        .. code-block:: python</span>

<span class="sd">            import ignite.distribted as idist</span>

<span class="sd">            model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)</span>
<span class="sd">            model = idist.auto_model(model)</span>

<span class="sd">    .. _torch DistributedDataParallel: https://pytorch.org/docs/stable/generated/torch.nn.parallel.</span>
<span class="sd">        DistributedDataParallel.html</span>
<span class="sd">    .. _torch DataParallel: https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html</span>
<span class="sd">    .. _torch convert_sync_batchnorm: https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#</span>
<span class="sd">        torch.nn.SyncBatchNorm.convert_sync_batchnorm</span>

<span class="sd">    .. versionchanged:: 0.4.2</span>

<span class="sd">        - Added Horovod distributed framework.</span>
<span class="sd">        - Added ``sync_bn`` argument.</span>

<span class="sd">    .. versionchanged:: 0.4.3</span>
<span class="sd">        Added kwargs to ``idist.auto_model``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">setup_logger</span><span class="p">(</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.auto_model&quot;</span><span class="p">)</span>

    <span class="c1"># Put model&#39;s parameters to device if its parameters are not on the device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">device</span> <span class="o">==</span> <span class="n">device</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># distributed data parallel model</span>
    <span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">bnd</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_native_dist_support</span> <span class="ow">and</span> <span class="n">bnd</span> <span class="ow">in</span> <span class="p">(</span><span class="n">idist_native</span><span class="o">.</span><span class="n">NCCL</span><span class="p">,</span> <span class="n">idist_native</span><span class="o">.</span><span class="n">GLOO</span><span class="p">,</span> <span class="n">idist_native</span><span class="o">.</span><span class="n">MPI</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">sync_bn</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Convert batch norm to sync batch norm&quot;</span><span class="p">)</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SyncBatchNorm</span><span class="o">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
                <span class="k">if</span> <span class="s2">&quot;device_ids&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument kwargs should not contain &#39;device_ids&#39;, but got </span><span class="si">{</span><span class="n">kwargs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

                <span class="n">lrank</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">get_local_rank</span><span class="p">()</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Apply torch DistributedDataParallel on model, device id: </span><span class="si">{</span><span class="n">lrank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;device_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">lrank</span><span class="p">,</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Apply torch DistributedDataParallel on model&quot;</span><span class="p">)</span>

            <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_hvd_support</span> <span class="ow">and</span> <span class="n">bnd</span> <span class="o">==</span> <span class="n">idist_hvd</span><span class="o">.</span><span class="n">HOROVOD</span><span class="p">:</span>
            <span class="kn">import</span><span class="w"> </span><span class="nn">horovod.torch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hvd</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Broadcast the initial variable states from rank 0 to all other processes&quot;</span><span class="p">)</span>
            <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_parameters</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># not distributed but multiple GPUs reachable so data parallel model</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="s2">&quot;cuda&quot;</span> <span class="ow">in</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span><span class="o">.</span><span class="n">type</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Apply torch DataParallel on model&quot;</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span></div>


<div class="viewcode-block" id="auto_optim"><a class="viewcode-back" href="../../../generated/ignite.distributed.auto.auto_optim.html#ignite.distributed.auto.auto_optim">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">auto_optim</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optimizer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper method to adapt optimizer for non-distributed and distributed configurations (supporting</span>
<span class="sd">    all available backends from :meth:`~ignite.distributed.utils.available_backends()`).</span>

<span class="sd">    Internally, this method is no-op for non-distributed and torch native distributed configuration.</span>

<span class="sd">    For XLA distributed configuration, we create a new class that inherits from provided optimizer.</span>
<span class="sd">    The goal is to override the `step()` method with specific `xm.optimizer_step`_ implementation.</span>

<span class="sd">    For Horovod distributed configuration, optimizer is wrapped with Horovod Distributed Optimizer and</span>
<span class="sd">    its state is broadcasted from rank 0 to all other processes.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer: input torch optimizer</span>
<span class="sd">        kwargs: kwargs to Horovod backend&#39;s DistributedOptimizer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optimizer</span>

<span class="sd">    Examples:</span>
<span class="sd">        .. code-block:: python</span>

<span class="sd">            import ignite.distributed as idist</span>

<span class="sd">            optimizer = idist.auto_optim(optimizer)</span>

<span class="sd">    .. _xm.optimizer_step: https://pytorch.org/xla/release/1.5/index.html#torch_xla.core.xla_model.optimizer_step</span>

<span class="sd">    .. versionchanged:: 0.4.2</span>
<span class="sd">        Added Horovod distributed optimizer.</span>

<span class="sd">    .. versionchanged:: 0.4.7</span>
<span class="sd">        Added kwargs to ``idist.auto_optim``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bnd</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_xla_support</span> <span class="ow">and</span> <span class="n">bnd</span> <span class="o">==</span> <span class="n">idist_xla</span><span class="o">.</span><span class="n">XLA_TPU</span><span class="p">:</span>
        <span class="bp">cls</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,),</span> <span class="nb">dict</span><span class="p">(</span><span class="n">_XLADistributedOptimizer</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_hvd_support</span> <span class="ow">and</span> <span class="n">bnd</span> <span class="o">==</span> <span class="n">idist_hvd</span><span class="o">.</span><span class="n">HOROVOD</span><span class="p">:</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">horovod.torch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">hvd</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">hvd</span><span class="o">.</span><span class="n">DistributedOptimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">hvd</span><span class="o">.</span><span class="n">broadcast_optimizer_state</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">root_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer</span>

    <span class="k">return</span> <span class="n">optimizer</span></div>


<div class="viewcode-block" id="DistributedProxySampler"><a class="viewcode-back" href="../../../generated/ignite.distributed.auto.DistributedProxySampler.html#ignite.distributed.auto.DistributedProxySampler">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">DistributedProxySampler</span><span class="p">(</span><span class="n">DistributedSampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Distributed sampler proxy to adapt user&#39;s sampler for distributed data parallelism configuration.</span>

<span class="sd">    Code is based on https://github.com/pytorch/pytorch/issues/23430#issuecomment-562350407</span>

<span class="sd">    Args:</span>
<span class="sd">        sampler: Input torch data sampler.</span>
<span class="sd">        num_replicas: Number of processes participating in distributed training.</span>
<span class="sd">        rank: Rank of the current process within ``num_replicas``.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Input sampler is assumed to have a constant size.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sampler</span><span class="p">:</span> <span class="n">Sampler</span><span class="p">,</span> <span class="n">num_replicas</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">Sampler</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument sampler should be instance of torch Sampler, but given: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">sampler</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">DistributedSampler</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument sampler must not be a distributed sampler already&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="s2">&quot;__len__&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument sampler should have length&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">DistributedProxySampler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">sampler</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="n">num_replicas</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">:</span>
        <span class="c1"># deterministically shuffle based on epoch</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">)</span>

        <span class="n">indices</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">]</span>

        <span class="c1"># subsample</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">rank</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span></div>


<span class="k">if</span> <span class="n">idist</span><span class="o">.</span><span class="n">has_xla_support</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch_xla.core.xla_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xm</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch_xla.distributed.parallel_loader</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParallelLoader</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">_MpDeviceLoader</span><span class="p">:</span>
        <span class="c1"># https://github.com/pytorch/xla/pull/2117</span>
        <span class="c1"># From pytorch/xla if `torch_xla.distributed.parallel_loader.MpDeviceLoader` is not available</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loader</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loader</span> <span class="o">=</span> <span class="n">loader</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">device</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_loader_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>

        <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">:</span>
            <span class="n">parallel_loader</span> <span class="o">=</span> <span class="n">ParallelLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_loader</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">],</span> <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_parallel_loader_kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">parallel_loader</span><span class="o">.</span><span class="n">per_device_loader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_device</span><span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_loader</span><span class="p">)</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">_XLADistributedOptimizer</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>  <span class="c1"># type: ignore[call-arg]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">wrapped_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
            <span class="n">xm</span><span class="o">.</span><span class="n">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">wrapped_optimizer</span><span class="p">,</span> <span class="n">barrier</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <table>
      <tr>
        <td>
            <p>
                &copy; Copyright 2025, PyTorch-Ignite Contributors.
              Last updated on 10/16/2025, 3:49:43‚ÄØPM.
            </p>
            
              <div>
                Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
              </div>
          
        </td>
        <td>
            
              <div>
                <a href="https://www.netlify.com" target="_blank" rel="noopener noreferrer">
                  <img
                  src="_static/img/netlify-light.svg"
                  alt="Deploys by Netlify"
                  width=114
                  height=51 />
                </a>
              </div>
            
        </td>
      </tr>
    </table>
  </div> 

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/sphinx_highlight.js"></script>
         <script src="../../../_static/clipboard.min.js"></script>
         <script src="../../../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Show default setup';</script>
         <script>let toggleHintHide = 'Hide default setup';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../../../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
         <script src="../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- commented out for Ignite -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <!-- <div class="container">
      <div class="row">
        <div class="text-center col-md-4">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/ignite/index.html">View Docs</a>
        </div>

        <div class="text-center col-md-4">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="">View Tutorials</a>
        </div>

        <div class="text-center col-md-4">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="">View Resources</a>
        </div>
      </div>
    </div> -->
  </div>

  <!-- <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-ignite.ai" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-ignite.ai">PyTorch</a></li>
            <li><a href="">Get Started</a></li>
            <li><a href="">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="">Blog</a></li>
            <li><a href="">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Support</a></li>
            <li><a href="">Tutorials</a></li>
            <li><a href="https://pytorch.org/ignite/index.html">Docs</a></li>
            <li><a href="" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/ignite/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/ignite/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!~~ real people should not fill this in and expect good things - do not remove this or risk form bot signups ~~>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer> -->


  <!-- end of commented out for Ignite -->

  <!-- <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook‚Äôs Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div> -->

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->
  <!--
  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-ignite.ai" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="">Blog</a>
          </li>

          <li>
            <a href="">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/ignite/index.html">Docs</a>
          </li>

          <li>
            <a href="">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/ignite">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  -->
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    var collapsedSections = []
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
  <script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script>
  <script type="text/javascript">
  let VERSION
  if ('master (0f961480)'.startsWith('v')) {
    VERSION = 'master (0f961480)'
  } else {
    VERSION = 'master'
  }
  docsearch({
    container: '#docsearch',
    appId: '7EWYE1JCT3',
    apiKey: '841e93e60c16975ba1bd8c7c716eed82',
    indexName: 'pytorch-ignite',
    placeholder: 'Search PyTorch-Ignite docs',
    searchParameters: {
      facetFilters: [`version:${VERSION}`, 'tags:API-reference'],
    }
  });
  </script>
</body>
</html>