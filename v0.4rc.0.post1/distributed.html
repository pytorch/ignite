



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="author" content="PyTorch-Ignite Contributors">
  <meta name="creator" content="PyTorch-Ignite Contributors">
  <meta name="publisher" content="PyTorch-Ignite Contributors">
  <meta name="generator" content="Sphinx 5.3.0">
  <meta name="description" content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.">
  <meta name="keywords" content="pytorch, pytorch ignite, ignite, engine, events, handlers, metrics">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#ee4c2c">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@pytorch_ignite">
  <meta name="twitter:creator" content="@pytorch_ignite">
  <meta name="twitter:description" content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.">
  <meta name="twitter:title" content="ignite.distributed &mdash; PyTorch-Ignite v0.4rc.0.post1 Documentation">
  <meta name="twitter:image" content="https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logo.png">
  <meta name="twitter:image:alt" content="PyTorch-Ignite logo">

  <meta property="og:title" content="ignite.distributed &mdash; PyTorch-Ignite v0.4rc.0.post1 Documentation">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://pytorch.org/ignite/">
  <meta property="og:site_name" content="PyTorch-Ignite">
  <meta property="og:image" content="https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logo.png">
  <meta property="og:image:alt" content="PyTorch-Ignite logo">
  <meta property="og:description" content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.">

  <link rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin />

  
  <title>ignite.distributed &mdash; PyTorch-Ignite v0.4rc.0.post1 Documentation</title>
  

  
  
    <link rel="shortcut icon" type="image/svg+xml" href="_static/ignite_logomark.svg"/>
  
  
  
    <link rel="canonical" href="https://pytorch.org/ignite/distributed.html"/>
  

  

  
  
    

  

  <link rel="preload" href="_static/css/theme.css" as="style" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="preload" href="_static/pygments.css" as="style" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="preload" href="_static/css/theme.css" as="style" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="preload" href="_static/copybutton.css" as="style" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
  <link rel="preload" href="_static/togglebutton.css" as="style" />
  <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
  <link rel="preload" href="_static/banner.css" as="style" />
  <link rel="stylesheet" href="_static/banner.css" type="text/css" />
  <link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" as="style" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" type="text/css" />
  <link rel="preload" href="_static/katex-math.css" as="style" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
  <link rel="preload" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" as="style" />
  <link rel="stylesheet" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
    <link rel="preload" href="_static/css/ignite_theme.css" as="style" />
    <link rel="stylesheet" href="_static/css/ignite_theme.css" type="text/css" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3" as="style" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@3" type="text/css" />
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ignite.exceptions" href="exceptions.html" />
    <link rel="prev" title="ignite.metrics" href="metrics.html" /> 

  <!-- katex links / this needs to be loaded before fonts.html -->

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.css" crossorigin="anonymous">
<script async src="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/katex.min.js" crossorigin="anonymous"></script>

<!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.13.20/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5FELLLFHCP"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5FELLLFHCP');
  </script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">

    <a class="header-logo" href="/ignite" aria-label="PyTorch-Ignite"></a>

    <div class="header-container">

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch-ignite.ai/tutorials/beginner/01-getting-started/">Quickstart</a>
          </li>

          <li>
            <a href="https://pytorch-ignite.ai/concepts/">Concepts</a>
          </li>

          <!-- <li>
            <a href="https://pytorch-ignite.ai/tutorials/beginner/01-getting-started/#complete-code">Examples</a>
          </li> -->

          <li>
            <a href="https://pytorch-ignite.ai/how-to-guides/">FAQ</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/ignite" target="_blank" rel="noopener noreferrer">GitHub</a>
          </li>

          <li>
            <a href="https://pytorch-ignite.ai/about/community/">About us</a>
          </li>

          <li>
            <a href="https://pytorch-ignite.ai">⊳ pytorch-ignite.ai</a>
          </li>


        </ul>
      </div>

      <!-- <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a> -->
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <div class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></div>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  v0.4rc.0.post1
                </div>
              
            

            <div id="docsearch"></div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="concepts.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="engine.html">ignite.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="handlers.html">ignite.handlers</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">ignite.metrics</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ignite.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="exceptions.html">ignite.exceptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">ignite.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Contrib Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contrib/engines.html">ignite.contrib.engines</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib/metrics.html">ignite.contrib.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="contrib/handlers.html">ignite.contrib.handlers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Team</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="about.html">About us</a></li>
<li class="toctree-l1"><a class="reference internal" href="governance.html">PyTorch-Ignite governance</a></li>
</ul>

            
          
        </div>
      </div>

      <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        <span class="fa fa-book"> Other Versions</span>
        v: v0.4rc.0.post1
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl>
            <dt>Tags</dt>
            <dd><a href="../v0.3.0/index.html">v0.3.0</a></dd>
            <dd><a href="../v0.4.0.post1/distributed.html">v0.4.0.post1</a></dd>
            <dd><a href="../v0.4.1/distributed.html">v0.4.1</a></dd>
            <dd><a href="../v0.4.10/distributed.html">v0.4.10</a></dd>
            <dd><a href="../v0.4.11/distributed.html">v0.4.11</a></dd>
            <dd><a href="../v0.4.12/distributed.html">v0.4.12</a></dd>
            <dd><a href="../v0.4.13/distributed.html">v0.4.13</a></dd>
            <dd><a href="../v0.4.2/distributed.html">v0.4.2</a></dd>
            <dd><a href="../v0.4.3/distributed.html">v0.4.3</a></dd>
            <dd><a href="../v0.4.4.post1/distributed.html">v0.4.4.post1</a></dd>
            <dd><a href="../v0.4.5/distributed.html">v0.4.5</a></dd>
            <dd><a href="../v0.4.6/distributed.html">v0.4.6</a></dd>
            <dd><a href="../v0.4.7/distributed.html">v0.4.7</a></dd>
            <dd><a href="../v0.4.8/distributed.html">v0.4.8</a></dd>
            <dd><a href="../v0.4.9/distributed.html">v0.4.9</a></dd>
            <dd><a href="distributed.html">v0.4rc.0.post1</a></dd>
            <dd><a href="../v0.5.0.post2/distributed.html">v0.5.0.post2</a></dd>
            <dd><a href="../v0.5.1/distributed.html">v0.5.1</a></dd>
            <dd><a href="../v0.5.2/distributed.html">v0.5.2</a></dd>
            <dd><a href="../v0.5.3/distributed.html">v0.5.3</a></dd>
        </dl>
        <dl>
            <dt>Branches</dt>
            <dd><a href="../master/distributed.html">master</a></dd>
        </dl>
    </div>
</div>

    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>ignite.distributed</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/distributed.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="ignite-distributed">
<h1>ignite.distributed<a class="headerlink" href="#ignite-distributed" title="Permalink to this heading">#</a></h1>
<p>Helper module to use distributed settings for multiple backends:</p>
<ul class="simple">
<li><p>backends from native torch distributed configuration: “nccl”, “gloo”, “mpi”</p></li>
<li><p>XLA on TPUs via <a class="reference external" href="https://github.com/pytorch/xla">pytorch/xla</a></p></li>
</ul>
<p>This module wraps common methods to fetch information about distributed configuration, initialize/finalize process
group or spawn multiple processes.</p>
<p>Examples:</p>
<blockquote>
<div><ul class="simple">
<li><p>Example to spawn <cite>nprocs</cite> processes that run <cite>fn</cite> with <cite>args</cite>: <a class="reference internal" href="#ignite.distributed.spawn" title="ignite.distributed.spawn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">spawn()</span></code></a></p></li>
</ul>
</div></blockquote>
<span class="target" id="module-ignite.distributed"></span><dl class="py attribute">
<dt class="sig sig-object py" id="ignite.distributed.has_xla_support">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">has_xla_support</span></span><a class="headerlink" href="#ignite.distributed.has_xla_support" title="Permalink to this definition">#</a></dt>
<dd><p>True if <cite>torch_xla</cite> package is found</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.all_gather">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#all_gather"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.all_gather" title="Permalink to this definition">#</a></dt>
<dd><p>Helper method to perform all gather operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tensor</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em> or </em><em>number</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – tensor or number or str to collect across participating processes.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>torch.Tensor of shape <cite>(world_size * tensor.shape[0], tensor.shape[1], …)</cite> or
List of strings</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.14)"><em>Union</em></a>[<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>Tensor</em></a>, <a class="reference external" href="https://docs.python.org/3/library/numbers.html#numbers.Number" title="(in Python v3.14)"><em>Number</em></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.14)"><em>List</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)">str</a>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.all_reduce">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">all_reduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SUM'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#all_reduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.all_reduce" title="Permalink to this definition">#</a></dt>
<dd><p>Helper method to perform all reduce operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em> or </em><em>number</em>) – tensor or number to collect across participating processes.</p></li>
<li><p><strong>op</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – reduction operation, “SUM” by default. Possible values: “SUM”, “PRODUCT”, “MIN”, “MAX”, “AND”, “OR”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>torch.Tensor or number</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.14)"><em>Union</em></a>[<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>Tensor</em></a>, <a class="reference external" href="https://docs.python.org/3/library/numbers.html#numbers.Number" title="(in Python v3.14)"><em>Number</em></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.available_backends">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">available_backends</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#available_backends"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.available_backends" title="Permalink to this definition">#</a></dt>
<dd><p>Returns available backends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.14)"><em>Tuple</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.backend">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">backend</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#backend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.backend" title="Permalink to this definition">#</a></dt>
<dd><p>Returns computation model’s backend.</p>
<ul class="simple">
<li><p><cite>None</cite> for no distributed configuration</p></li>
<li><p>“nccl” or “gloo” or “mpi” for native torch distributed configuration</p></li>
<li><p>“xla-tpu” for XLA distributed configuration</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>str or None</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.14)"><em>Optional</em></a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)">str</a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.barrier">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">barrier</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#barrier"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.barrier" title="Permalink to this definition">#</a></dt>
<dd><p>Helper method to synchronize all processes.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.device">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#device"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.device" title="Permalink to this definition">#</a></dt>
<dd><p>Returns current device according to current distributed configuration.</p>
<ul class="simple">
<li><p><cite>torch.device(“cpu”)</cite> if no distributed configuration or native gloo distributed configuration</p></li>
<li><p><cite>torch.device(“cuda:local_rank”)</cite> if native nccl distributed configuration</p></li>
<li><p><cite>torch.device(“xla:index”)</cite> if XLA distributed configuration</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>torch.device</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.10)"><em>device</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.finalize">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">finalize</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#finalize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.finalize" title="Permalink to this definition">#</a></dt>
<dd><p>Finalizes distributed configuration. For example, in case of native pytorch distributed configuration,
it calls <code class="docutils literal notranslate"><span class="pre">dist.destroy_process_group()</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.get_local_rank">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">get_local_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#get_local_rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.get_local_rank" title="Permalink to this definition">#</a></dt>
<dd><p>Returns local process rank within current distributed configuration. Returns 0 if no distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.get_node_rank">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">get_node_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#get_node_rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.get_node_rank" title="Permalink to this definition">#</a></dt>
<dd><p>Returns node rank within current distributed configuration.
Returns 0 if no distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.get_ntasks_per_node">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">get_ntasks_per_node</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#get_ntasks_per_node"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.get_ntasks_per_node" title="Permalink to this definition">#</a></dt>
<dd><p>Returns number of processes (or tasks) per node within current distributed configuration.
Returns 1 if no distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.get_num_nodes">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">get_num_nodes</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#get_num_nodes"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.get_num_nodes" title="Permalink to this definition">#</a></dt>
<dd><p>Returns number of nodes within current distributed configuration.
Returns 1 if no distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.get_rank">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">get_rank</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#get_rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.get_rank" title="Permalink to this definition">#</a></dt>
<dd><p>Returns process rank within current distributed configuration. Returns 0 if no distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.get_world_size">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">get_world_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#get_world_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.get_world_size" title="Permalink to this definition">#</a></dt>
<dd><p>Returns world size of current distributed configuration. Returns 1 if no distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)">int</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.hostname">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">hostname</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#hostname"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.hostname" title="Permalink to this definition">#</a></dt>
<dd><p>Returns host name for current process within current distributed configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.initialize">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">initialize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#initialize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.initialize" title="Permalink to this definition">#</a></dt>
<dd><p>Initializes distributed configuration according to provided <code class="docutils literal notranslate"><span class="pre">backend</span></code></p>
<p class="rubric">Examples</p>
<p>Launch single node multi-GPU training with <code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> utility.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># &gt;&gt;&gt; python -m torch.distributed.launch --nproc_per_node=4 main.py</span>

<span class="c1"># main.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ignite.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">idist</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_fn</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
    <span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">local_rank</span><span class="p">))</span>


<span class="n">idist</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="n">local_rank</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">get_local_rank</span><span class="p">()</span>
<span class="n">train_fn</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">idist</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backend</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>) – backend: <cite>nccl</cite>, <cite>gloo</cite>, <cite>xla-tpu</cite>.</p></li>
<li><p><strong>**kwargs</strong> – <p>acceptable kwargs according to provided backend:</p>
<ul>
<li><p>”nccl” or “gloo” : timeout(=timedelta(minutes=30))</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.model_name">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">model_name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#model_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.model_name" title="Permalink to this definition">#</a></dt>
<dd><p>Returns distributed configuration name (given by ignite)</p>
<ul class="simple">
<li><p><cite>serial</cite> for no distributed configuration</p></li>
<li><p><cite>native-dist</cite> for native torch distributed configuration</p></li>
<li><p><cite>xla-dist</cite> for XLA distributed configuration</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)">str</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.one_rank_only">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">one_rank_only</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_barrier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#one_rank_only"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.one_rank_only" title="Permalink to this definition">#</a></dt>
<dd><p>Decorator to filter handlers wrt a rank number</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – rank number of the handler (default: 0).</p></li>
<li><p><strong>with_barrier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>) – synchronisation with a barrier (default: False).</p></li>
</ul>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">engine</span> <span class="o">=</span> <span class="o">...</span>

<span class="nd">@engine</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="nd">@one_rank_only</span><span class="p">()</span> <span class="c1"># means @one_rank_only(rank=0)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">some_handler</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
    <span class="o">...</span>

<span class="nd">@engine</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="nd">@one_rank_only</span><span class="p">(</span><span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">some_handler</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.set_local_rank">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">set_local_rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#set_local_rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.set_local_rank" title="Permalink to this definition">#</a></dt>
<dd><p>Method to hint the local rank in case if torch native distributed context is created by user
without using <a class="reference internal" href="#ignite.distributed.initialize" title="ignite.distributed.initialize"><code class="xref py py-meth docutils literal notranslate"><span class="pre">initialize()</span></code></a> or <a class="reference internal" href="#ignite.distributed.spawn" title="ignite.distributed.spawn"><code class="xref py py-meth docutils literal notranslate"><span class="pre">spawn()</span></code></a>.</p>
<p>Usage:</p>
<blockquote>
<div><p>User set up torch native distributed process group</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">ignite.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">idist</span>

<span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

    <span class="n">idist</span><span class="o">.</span><span class="n">set_local_rank</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
    <span class="c1"># ...</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="o">**</span><span class="n">dist_info</span><span class="p">)</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – local rank or current process index</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.show_config">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">show_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#show_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.show_config" title="Permalink to this definition">#</a></dt>
<dd><p>Helper method to display distributed configuration via <code class="docutils literal notranslate"><span class="pre">logging</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.spawn">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">spawn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backend</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_procs_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#spawn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.spawn" title="Permalink to this definition">#</a></dt>
<dd><p>Spawns <code class="docutils literal notranslate"><span class="pre">num_procs_per_node</span></code> processes that run <code class="docutils literal notranslate"><span class="pre">fn</span></code> with <code class="docutils literal notranslate"><span class="pre">args</span></code>/<code class="docutils literal notranslate"><span class="pre">kwargs_dict</span></code> and initialize
distributed configuration defined by <code class="docutils literal notranslate"><span class="pre">backend</span></code>.</p>
<p class="rubric">Examples</p>
<ol class="arabic simple">
<li><p>Launch single node multi-GPU training</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># &gt;&gt;&gt; python main.py</span>

<span class="c1"># main.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ignite.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">idist</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_fn</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
    <span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">local_rank</span><span class="p">))</span>


<span class="n">idist</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="n">kwargs_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="mi">23</span><span class="p">},</span> <span class="n">num_procs_per_node</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Launch multi-node multi-GPU training</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># &gt;&gt;&gt; (node 0): python main.py --node_rank=0 --num_nodes=8 --master_addr=master --master_port=2222</span>
<span class="c1"># &gt;&gt;&gt; (node 1): python main.py --node_rank=1 --num_nodes=8 --master_addr=master --master_port=2222</span>
<span class="c1"># &gt;&gt;&gt; ...</span>
<span class="c1"># &gt;&gt;&gt; (node 7): python main.py --node_rank=7 --num_nodes=8 --master_addr=master --master_port=2222</span>

<span class="c1"># main.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">ignite.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">idist</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_fn</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_procs_per_node</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
    <span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">dist</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="n">num_nodes</span> <span class="o">*</span> <span class="n">num_procs_per_node</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">device</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">local_rank</span><span class="p">))</span>

<span class="n">idist</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
    <span class="s2">&quot;nccl&quot;</span><span class="p">,</span>
    <span class="n">train_fn</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">num_procs_per_node</span><span class="p">),</span>
    <span class="n">num_procs_per_node</span><span class="o">=</span><span class="n">num_procs_per_node</span><span class="p">,</span>
    <span class="n">num_nodes</span><span class="o">=</span><span class="n">num_nodes</span><span class="p">,</span>
    <span class="n">node_rank</span><span class="o">=</span><span class="n">node_rank</span><span class="p">,</span>
    <span class="n">master_addr</span><span class="o">=</span><span class="n">master_addr</span><span class="p">,</span>
    <span class="n">master_port</span><span class="o">=</span><span class="n">master_port</span>
<span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Launch single node multi-TPU training (for example on Google Colab)</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># &gt;&gt;&gt; python main.py</span>

<span class="c1"># main.py</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">ignite.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">idist</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_fn</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch_xla.core.xla_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">xm</span>
    <span class="k">assert</span> <span class="n">xm</span><span class="o">.</span><span class="n">get_world_size</span><span class="p">()</span> <span class="o">==</span> <span class="mi">8</span>

    <span class="n">device</span> <span class="o">=</span> <span class="n">idist</span><span class="o">.</span><span class="n">device</span><span class="p">()</span>
    <span class="k">assert</span> <span class="s2">&quot;xla&quot;</span> <span class="ow">in</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span>


<span class="n">idist</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="s2">&quot;xla-tpu&quot;</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">),</span> <span class="n">kwargs_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;d&quot;</span><span class="p">:</span> <span class="mi">23</span><span class="p">},</span> <span class="n">num_procs_per_node</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backend</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>) – backend to use: <cite>nccl</cite>, <cite>gloo</cite>, <cite>xla-tpu</cite></p></li>
<li><p><strong>fn</strong> (<em>function</em>) – function to called as the entrypoint of the spawned process.
This function must be defined at the top level of a module so it can be pickled and spawned.
This is a requirement imposed by multiprocessing. The function is called as <code class="docutils literal notranslate"><span class="pre">fn(i,</span> <span class="pre">*args,</span> <span class="pre">**kwargs_dict)</span></code>,
where <cite>i</cite> is the process index and args is the passed through tuple of arguments.</p></li>
<li><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><em>tuple</em></a>) – arguments passed to <cite>fn</cite>.</p></li>
<li><p><strong>kwargs_dict</strong> (<em>Mapping</em>) – kwargs passed to <cite>fn</cite>.</p></li>
<li><p><strong>num_procs_per_node</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>) – number of processes to spawn on a single node. Default, 1.</p></li>
<li><p><strong>**kwargs</strong> – <p>acceptable kwargs according to provided backend:</p>
<ul>
<li><div class="line-block">
<div class="line">”nccl” or “gloo” : <cite>num_nodes</cite> (default, 1), <cite>node_rank</cite> (default, 0), <cite>master_addr</cite></div>
<div class="line">(default, “127.0.0.1”), <cite>master_port</cite> (default, 2222), <cite>timeout</cite> to <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group">dist.init_process_group</a> function</div>
<div class="line">and kwargs for <a class="reference external" href="https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn">mp.spawn</a> function.</div>
</div>
</li>
<li><p>”xla-tpu” : <cite>num_nodes</cite> (default, 1), <cite>node_rank</cite> (default, 0) and kwargs to <a class="reference external" href="http://pytorch.org/xla/release/1.5/index.html#torch_xla.distributed.xla_multiprocessing.spawn">xmp.spawn</a> function.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="ignite.distributed.sync">
<span class="sig-prename descclassname"><span class="pre">ignite.distributed.</span></span><span class="sig-name descname"><span class="pre">sync</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/ignite/distributed/utils.html#sync"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#ignite.distributed.sync" title="Permalink to this definition">#</a></dt>
<dd><p>Helper method to force this module to synchronize with current distributed context.
This method should be used when distributed context is manually created or destroyed.</p>
</dd></dl>

</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="exceptions.html" class="btn btn-neutral float-right" title="ignite.exceptions" accesskey="n" rel="next">Next
          <img src="_static/images/chevron-right-orange.svg" alt="right arrow" class="next-page">
        </a>
      
      
        <a href="metrics.html" class="btn btn-neutral" title="ignite.metrics" accesskey="p" rel="prev">
          <img src="_static/images/chevron-right-orange.svg" alt="left arrow" class="previous-page"> Previous
        </a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <table>
      <tr>
        <td>
            <p>
                &copy; Copyright 2026, PyTorch-Ignite Contributors.
              Last updated on 05/31/2020, 11:07:58 PM.
            </p>
            
              <div>
                Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
              </div>
          
        </td>
        <td>
            
              <div>
                <a href="https://www.netlify.com" target="_blank" rel="noopener noreferrer">
                  <img
                  src="_static/img/netlify-light.svg"
                  alt="Deploys by Netlify"
                  width=114
                  height=51 />
                </a>
              </div>
            
        </td>
      </tr>
    </table>
  </div> 

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">ignite.distributed</a><ul>
<li><a class="reference internal" href="#ignite.distributed.has_xla_support"><code class="docutils literal notranslate"><span class="pre">has_xla_support</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.all_gather"><code class="docutils literal notranslate"><span class="pre">all_gather()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.all_reduce"><code class="docutils literal notranslate"><span class="pre">all_reduce()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.available_backends"><code class="docutils literal notranslate"><span class="pre">available_backends()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.backend"><code class="docutils literal notranslate"><span class="pre">backend()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.barrier"><code class="docutils literal notranslate"><span class="pre">barrier()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.device"><code class="docutils literal notranslate"><span class="pre">device()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.finalize"><code class="docutils literal notranslate"><span class="pre">finalize()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.get_local_rank"><code class="docutils literal notranslate"><span class="pre">get_local_rank()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.get_node_rank"><code class="docutils literal notranslate"><span class="pre">get_node_rank()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.get_ntasks_per_node"><code class="docutils literal notranslate"><span class="pre">get_ntasks_per_node()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.get_num_nodes"><code class="docutils literal notranslate"><span class="pre">get_num_nodes()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.get_rank"><code class="docutils literal notranslate"><span class="pre">get_rank()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.get_world_size"><code class="docutils literal notranslate"><span class="pre">get_world_size()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.hostname"><code class="docutils literal notranslate"><span class="pre">hostname()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.initialize"><code class="docutils literal notranslate"><span class="pre">initialize()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.model_name"><code class="docutils literal notranslate"><span class="pre">model_name()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.one_rank_only"><code class="docutils literal notranslate"><span class="pre">one_rank_only()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.set_local_rank"><code class="docutils literal notranslate"><span class="pre">set_local_rank()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.show_config"><code class="docutils literal notranslate"><span class="pre">show_config()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.spawn"><code class="docutils literal notranslate"><span class="pre">spawn()</span></code></a></li>
<li><a class="reference internal" href="#ignite.distributed.sync"><code class="docutils literal notranslate"><span class="pre">sync()</span></code></a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/sphinx_highlight.js"></script>
         <script src="_static/clipboard.min.js"></script>
         <script src="_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Show default setup';</script>
         <script>let toggleHintHide = 'Hide default setup';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
         <script src="_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- commented out for Ignite -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <!-- <div class="container">
      <div class="row">
        <div class="text-center col-md-4">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/ignite/index.html">View Docs</a>
        </div>

        <div class="text-center col-md-4">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="">View Tutorials</a>
        </div>

        <div class="text-center col-md-4">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="">View Resources</a>
        </div>
      </div>
    </div> -->
  </div>

  <!-- <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch-ignite.ai" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch-ignite.ai">PyTorch</a></li>
            <li><a href="">Get Started</a></li>
            <li><a href="">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="">Blog</a></li>
            <li><a href="">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="">Support</a></li>
            <li><a href="">Tutorials</a></li>
            <li><a href="https://pytorch.org/ignite/index.html">Docs</a></li>
            <li><a href="" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/ignite/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/ignite/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!~~ real people should not fill this in and expect good things - do not remove this or risk form bot signups ~~>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer> -->


  <!-- end of commented out for Ignite -->

  <!-- <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div> -->

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->
  <!--
  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch-ignite.ai" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="">Blog</a>
          </li>

          <li>
            <a href="">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/ignite/index.html">Docs</a>
          </li>

          <li>
            <a href="">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/ignite">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  -->
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    var collapsedSections = []
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
  <script src="https://cdn.jsdelivr.net/npm/@docsearch/js@3"></script>
  <script type="text/javascript">
  let VERSION
  if ('v0.4rc.0.post1'.startsWith('v')) {
    VERSION = 'v0.4rc.0.post1'
  } else {
    VERSION = 'master'
  }
  docsearch({
    container: '#docsearch',
    appId: '7EWYE1JCT3',
    apiKey: '841e93e60c16975ba1bd8c7c716eed82',
    indexName: 'pytorch-ignite',
    placeholder: 'Search PyTorch-Ignite docs',
    searchParameters: {
      facetFilters: [`version:${VERSION}`, 'tags:API-reference'],
    }
  });
  </script>
</body>
</html>