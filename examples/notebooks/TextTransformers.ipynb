{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48206fce",
      "metadata": {
        "id": "48206fce"
      },
      "source": [
        "# Fine-Tuning a Transformer (DistilBERT) for Text Classification with Ignite\n",
        "\n",
        "This tutorial demonstrates how to fine-tune a pre-trained Transformer model for text classification using PyTorch Ignite.\n",
        "\n",
        "Transformer models, like BERT and DistilBERT, use a mechanism called **Self-Attention**  to understand the contextual meaning of words across entire sentences, making them highly effective for Natural Language Processing (NLP) tasks.\n",
        "\n",
        "In this notebook, we will classify IMDB movie reviews to predict whether a review is positive or negative. We will use:\n",
        "* **Hugging Face `transformers`**: To load the pre-trained DistilBERT model and Tokenizer.\n",
        "* **Hugging Face `datasets`**: To efficiently download and process the IMDB dataset.\n",
        "* **PyTorch Ignite**: To manage the training loop, metrics, early stopping, and model checkpointing without writing boilerplate code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f82e2f",
      "metadata": {
        "id": "58f82e2f"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-ignite datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f421b9",
      "metadata": {
        "id": "b2f421b9"
      },
      "source": [
        "## 1. Import Libraries & Setup\n",
        "\n",
        "We begin by importing the necessary modules. We also configure our `device` to utilize a GPU if available, which significantly accelerates Transformer training. Finally, we set a manual seed to ensure our results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5338d83",
      "metadata": {
        "id": "b5338d83"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import load_dataset\n",
        "\n",
        "from ignite.engine import Engine, Events, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping, global_step_from_engine\n",
        "from ignite.contrib.handlers import ProgressBar\n",
        "from ignite.utils import manual_seed\n",
        "\n",
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cad445",
      "metadata": {
        "id": "26cad445"
      },
      "source": [
        "## 2. Data Processing\n",
        "\n",
        "Preparing text data for a Transformer involves three main steps:\n",
        "1. **Loading Data:** We stream the IMDB dataset using `load_dataset`.\n",
        "2. **Tokenization:** Neural networks cannot read raw text. We use `AutoTokenizer` to convert our text strings into integer token IDs that map to the model's vocabulary.\n",
        "3. **Dynamic Padding:** Models process data in batches, and all sequences in a batch must have the same length. Instead of padding every review to the absolute maximum length of the dataset, `DataCollatorWithPadding` dynamically pads sequences to the maximum length of the *current batch*. This saves significant memory and compute time. It also automatically generates the `attention_mask`, which tells the model which tokens are actual words and which are just padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5155062e",
      "metadata": {
        "id": "5155062e"
      },
      "outputs": [],
      "source": [
        "# 1. Load the IMDB dataset\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "\n",
        "# 2. Load Tokenizer\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# 3. Tokenize function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "# 4. Apply tokenization to the entire dataset\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# 5. Format for PyTorch\n",
        "# We remove the raw text and rename 'label' to 'labels' as expected by HF models\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# 6. Data Collator (Handles dynamic padding)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 7. Create DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=32, collate_fn=data_collator\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"test\"], batch_size=32, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec60f44",
      "metadata": {
        "id": "bec60f44"
      },
      "source": [
        "## 3. Model Setup\n",
        "\n",
        "We instantiate `AutoModelForSequenceClassification`. This downloads the pre-trained DistilBERT architecture and its weights, but replaces the original top layer with a randomly initialized classification head tailored for our task (binary classification, hence `num_labels=2`).\n",
        "\n",
        "We also initialize the `AdamW` optimizer, which is the standard choice for fine-tuning Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfda203d",
      "metadata": {
        "id": "bfda203d"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e807576",
      "metadata": {
        "id": "4e807576"
      },
      "source": [
        "## 4. Training Loop with Ignite\n",
        "\n",
        "Here we define the core `process_function` (for training) and `eval_function` (for validation) that Ignite's `Engine` will run.\n",
        "\n",
        "* **Dictionary Inputs:** Hugging Face models expect a dictionary of inputs (e.g., `input_ids`, `attention_mask`) and automatically calculate the loss internally if `labels` are provided. We unpack our batch directly into the model: `model(**batch)`.\n",
        "* **Automatic Mixed Precision (AMP):** We use `torch.amp.autocast()` and `GradScaler` to run operations in half-precision (`float16`) where safe. This dramatically speeds up training on modern GPUs while reducing memory footprint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756b921c",
      "metadata": {
        "id": "756b921c"
      },
      "outputs": [],
      "source": [
        "# Using Mixed Precision for speed\n",
        "scaler = GradScaler()\n",
        "\n",
        "def process_function(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Move batch dictionary to device\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "    # Forward pass with AMP\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "    # Backward pass\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def eval_function(engine, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Return (y_pred, y) for Ignite metrics to consume\n",
        "        return logits, batch[\"labels\"]\n",
        "\n",
        "# Instantiate Engines\n",
        "trainer = Engine(process_function)\n",
        "train_evaluator = Engine(eval_function)\n",
        "validation_evaluator = Engine(eval_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0434e51",
      "metadata": {
        "id": "d0434e51"
      },
      "source": [
        "## 5. Metrics and Handlers\n",
        "\n",
        "PyTorch Ignite's event-driven system allows us to attach metrics and handlers without cluttering our training loop.\n",
        "\n",
        "* **RunningAverage:** Smooths the loss output during training for easier tracking.\n",
        "* **Metrics:** We attach `Accuracy` and `Loss` to our evaluators to measure model performance at the end of each epoch.\n",
        "* **ProgressBar:** Provides a visual tqdm progress bar.\n",
        "* **EarlyStopping:** Stops training early if the validation accuracy does not improve for 2 consecutive epochs, preventing overfitting.\n",
        "* **ModelCheckpoint:** Automatically saves the best-performing model weights to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c324875a",
      "metadata": {
        "id": "c324875a"
      },
      "outputs": [],
      "source": [
        "# 1. Running Average of Loss\n",
        "RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')\n",
        "\n",
        "# 2. Accuracy and Loss Metrics\n",
        "metrics = {\n",
        "    'accuracy': Accuracy(),\n",
        "    'nll': Loss(torch.nn.CrossEntropyLoss())\n",
        "}\n",
        "\n",
        "for name, metric in metrics.items():\n",
        "    metric.attach(train_evaluator, name)\n",
        "    metric.attach(validation_evaluator, name)\n",
        "\n",
        "# 3. Progress Bar\n",
        "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
        "pbar.attach(trainer, ['loss'])\n",
        "\n",
        "eval_pbar = ProgressBar(desc=\"Evaluating\", persist=False)\n",
        "eval_pbar.attach(validation_evaluator)\n",
        "\n",
        "# 4. Log Validation Results at the end of every epoch\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(engine):\n",
        "    validation_evaluator.run(test_dataloader)\n",
        "    metrics = validation_evaluator.state.metrics\n",
        "    avg_accuracy = metrics['accuracy']\n",
        "    avg_nll = metrics['nll']\n",
        "\n",
        "    pbar.log_message(\n",
        "        f\"Validation Results - Epoch: {engine.state.epoch}  \"\n",
        "        f\"Avg accuracy: {avg_accuracy:.2f} Avg loss: {avg_nll:.2f}\"\n",
        "    )\n",
        "\n",
        "# 5. Early Stopping\n",
        "def score_function(engine):\n",
        "    return engine.state.metrics['accuracy']\n",
        "\n",
        "handler = EarlyStopping(patience=2, score_function=score_function, trainer=trainer)\n",
        "validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n",
        "\n",
        "# 6. Model Checkpoint\n",
        "checkpointer = ModelCheckpoint(\n",
        "    dirname='/tmp/models',\n",
        "    filename_prefix='distilbert_imdb',\n",
        "    n_saved=1,\n",
        "    create_dir=True,\n",
        "    require_empty=False,\n",
        "    score_function=score_function,\n",
        "    score_name=\"val_acc\",\n",
        "    global_step_transform=global_step_from_engine(trainer)\n",
        ")\n",
        "\n",
        "validation_evaluator.add_event_handler(Events.COMPLETED, checkpointer, {'model': model})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad9892ab",
      "metadata": {
        "id": "ad9892ab"
      },
      "source": [
        "## 6. Run Training\n",
        "\n",
        "Finally, we execute the training run. Because DistilBERT is already pre-trained on a massive corpus of text, it already understands language structure. We are simply \"fine-tuning\" it for sentiment analysis, which typically converges to high accuracy in just 2 to 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe966993",
      "metadata": {
        "id": "fe966993"
      },
      "outputs": [],
      "source": [
        "# Run for 3 epochs\n",
        "trainer.run(train_dataloader, max_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RFtQcBJvDo6"
      },
      "id": "1RFtQcBJvDo6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
